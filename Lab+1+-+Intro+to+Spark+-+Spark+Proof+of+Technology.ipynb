{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Hello Spark\n",
    "This lab will introduce you to Apache Spark.  It will be written in Python and run in IBM's Data Science Experience environment through a Jupyter notebook.  While you work, it will be valuable to reference the [Apache Spark Documentation](http://spark.apache.org/docs/latest/programming-guide.html).  Since it is Python, be careful of whitespace!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Working with Spark Context\n",
    "Step 1.1 - Invoke the spark context: sc.  The version method will return the working version of Apache Spark<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 1 - Check spark version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Working with Resilient Distributed Datasets\n",
    "\n",
    "Step 2.1 - Create an RDD with numbers 1 to 10\n",
    "\n",
    "Type: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; x_nbr_rdd = sc.parallelize(x)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.1 - Create RDD of numbers 1-10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.2 - Return the first element<br><br>\n",
    "Type: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.2 - Return first element\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.3 - Return an array of the first five elements<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.3 - Return an array of the first five elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.4 - Perform a map transformation to increment each element of the array.  The map function creates a new RDD by applying the function provided in the argument to each element.  For more information go to [Transformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations)<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 2.4 - Write your map function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.5 - Note that there was no result for step 2.4.  Why was this?  Take a look at all the elements of the new RDD.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; x_nbr_rdd_2.collect()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.5 - Check out the elements of the new RDD. Warning: Be careful with this in real life! Collect returns everything!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.6 - Create a new RDD with one string \"Hello Spark\" and print it.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; y_str_rdd = sc.parallelize(y)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; y_str_rdd.first()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.6 - Create a string y, then turn it into an RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.7 - Create a third RDD with several strings.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z = ['IBM Data Science Experience is built for enterprise-scale deployment.', \"Manage your data, your analytical assets, and your projects in a secured cloud environment.\" , \"When you create an account in the IBM Data Science Experience, we deploy for you a Spark as a Service instance to power your analysis and 5 GB of IBM Object Storage to store your data.\"]<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd = sc.parallelize(z)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.7 - Create String RDD with many lines / entries, Extract first line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.8 - Count the number of entries in this RDD.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.8 - Count the number of entries in the RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.9 - Inspect the elements of this RDD.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;z_str_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.9 - Show all the entries in the RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.10 - Split all the entries in the RDD on the spaces.  Then print it out.  Pay careful attention to the new format.<br><br>\n",
    "Type: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;z_str_rdd_split = z_str_rdd.map(lambda line: line.split(\" \"))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;z_str_rdd_split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 2.10 - Perform a map transformation to split all entries in the RDD\n",
    "#Check out the entries in the new RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.11 - Explore a new transformation: flatMap <br>\n",
    "flatMap will \"flatten\" all the elements of an RDD element into 0 or more output terms.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\" \"))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd_split_flatmap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 2.11 - Learn the difference between two transformations: map and flatMap.\n",
    "\n",
    "#What do you notice? How are the outputs of 2.10 and 2.11 different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.12 - Augment each entry in the previous RDD with the number \"1\" to create pairs or tuples. The first element of the tuple will be the word and the second elements of the tuple will be the digit \"1\".  This is a common step in performing a count.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; countWords.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.12 - Create pairs or tuple RDD and print it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.13 Now we have above what is known as a [Pair RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions). Each entry in the RDD has a KEY and a VALUE.<br>\n",
    "The KEY is the word (Light, of, the, ...) and the value is the number \"1\".  \n",
    "We can now AGGREGATE this RDD by summing up all the values BY KEY<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;countWords2 = countWords.reduceByKey(lambda x,y: x+y)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;countWords2.collect()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 2.13 - Check out the results of the aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Reading a file and counting words<br>\n",
    "<br>\n",
    "Step 3.1 - Read the Apache Spark Readme.md file from Github.  The ! gives you file system commands<br><br>\n",
    "Type:<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;!rm README.md* -f<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;!wget https://raw.githubusercontent.com/apache/spark/master/README.md<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 3.1 - Pull data file into workbench\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.2 - Create an RDD by reading from the local filesystem.  Here is the [textfile documentation](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext.textFile).  Print the count to check that the read was successful.<br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;textfile_rdd = sc.textFile(\"README.md\")<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;textfile_rdd.count()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 3.2 - Create RDD from data file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.3<br>Filter out lines that contain \"Spark\". This will be achieved using the [filter](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=filter#pyspark.RDD.filter) transformation.  Python allows us to use the 'in' syntax to search strings.<br>\n",
    "We will also take a look at the first line in the newly filtered RDD. <br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Spark_lines.first()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 3.3 - Filter for only lines with word Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.4 - Count the number of entries in this filtered RDD and print the result as a concatenated string.<br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;print \"The file README.md has \" + str(Spark_lines.count()) + \\<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\" of \" + str(textfile_rdd.count()) + \\<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\" lines with the word Spark in it.\"<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 3.4 - count the number of lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.5 - Now count the number of times the word Spark appears in the original text, not just the number of lines that contain it.  <br>\n",
    "Instructions:<br>\n",
    "Looking back at previous exercises, you will need to: <br>\n",
    "1 - Execute a flatMap transformation on the original RDD Spark_lines and split on white space.<br>\n",
    "2 - Filter out all instances of the word Spark<br>\n",
    "3 - Count all instances\n",
    "4 - Print the total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 3.5 - Count the number of instances of tokens starting with \"Spark\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Perform analysis on a data file\n",
    "This part is a little more open ended and there are a few ways to complete it.  Scroll up to previous examples for some guidance.  You will download a data file, transform the data, and then average the prices.  The data file will be a sample of tech stock prices over six days. <br>\n",
    "\n",
    "Data Location: https://raw.githubusercontent.com/JosephKambourakisIBM/SparkPoT/master/StockPrices.csv<br>\n",
    "The data file is a csv<br>\n",
    "Here is a sample of the file:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"IBM\",\"159.720001\" ,\"159.399994\" ,\"158.880005\",\"159.539993\", \"159.550003\", \"160.350006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 4.1 - Delete the file if it exists, download a new copy and load it into an RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 4.2 - Transform the data to extract the stock ticker symbol and the prices.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 4.3 - Compute the averages and print them for each symbol.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}