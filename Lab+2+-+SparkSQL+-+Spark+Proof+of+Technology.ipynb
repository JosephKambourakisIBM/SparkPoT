{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Spark SQL\n",
    "This lab will show you how to work with SparkSQL.  It's meant to be self-guided, but don't hesitate to ask your presentor for help.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Getting started: Create a SQL Context\n",
    "\n",
    "Type:<br>\n",
    "\n",
    "from pyspark.sql import SQLContext<br>\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the SparkSQL library and start the connection to Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Download a JSON Recordset to work with\n",
    "Let's download the data, we can run commands on the console of the server (or docker image) that the notebook environment is using. To do so we simply put a \"!\" in front of the command that we want to run. For example:\n",
    "\n",
    "!pwd\n",
    "\n",
    "To get the data we will download a file to the environment. Simple run these two commands, the first just ensures that the file is removed if it exists:\n",
    "\n",
    "!rm world_bank.json.gz -f <br>\n",
    "!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Download file here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Create a Dataframe \n",
    "\n",
    "Now you can create the Dataframe, note that if you wanted to see where you downloaded the file you can run !pwd or !ls\n",
    "\n",
    "To create the Dataframe type:<br>\n",
    "\n",
    "example1_df = sqlContext.read.json(\"world_bank.json.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create the Dataframe here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>We can look at the schema with this command:</h3>\n",
    "\n",
    "Type: <br>\n",
    "example1_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Print out the schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes are a subset of RDDs and can be similarly transformed.  You can map and filter them.\n",
    "<br>Take a look at the first two rows of data using the [take()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=take#pyspark.sql.DataFrame.take) function.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use take on the DataFrame to pull out 2 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Register a Table\n",
    "The function is: DataframeObject.registerTempTable(\"name_of_table\").\n",
    "<br>\n",
    "Create a table named \"world_bank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create the table to be referenced via SparkSQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Writing SQL Statements\n",
    "\n",
    "Using SQL Get the first 2 records\n",
    "sqlContext.sql(\"SQL Statement\") will return a Dataframe with the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use SQL to query the table and print the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extra credit, take the DataFrame you created with the two records and convert it into a Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now calculate a simple count based on a group, for example \"regionname\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With JSON data you can reference the nested data.  \n",
    "# If you look at Schema above you can see that sector.Name is a nested column.\n",
    "# Select that column and limit to reasonable output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Creating Simple Graphs\n",
    "\n",
    "Create some simple graphs using the Pandas library\n",
    "#### First create a SQL statement that is a reasonable number of items\n",
    "For example, you can count the number of projects (rows) by countryname\n",
    "<br>or in other words: \n",
    "<br>count(*), countryname from table group by countryname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Write the sql statement and look at the data, remember to add .toPandas() for a formatted display. An easier option is to create a variable and set it to the SQL statement.\n",
    "For example:<br>\n",
    "\n",
    "query = \"select count(*) as Count, countryname from world_bank group by countryname\"\n",
    "chart1_df = sqlContext.sql(query).toPandas()\n",
    "print chart1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now take the variable (or same sql statement) and use the method:\n",
    "# .plot(kind='bar', x='countryname', y='Count', figsize=(12, 5)) to plot a graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Creating a DataFrame \n",
    "Try adding a schema to an RDD to create a DataFrame.<br>\n",
    "First, you need to create an RDD. This can be done with a loop or as\n",
    "seen in the instructor's example, or more simply by assigning values to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Default array defined below. Feel free to change as desired.\n",
    "array=[[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5]]\n",
    "my_rdd = sc.parallelize(array)\n",
    "my_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, you need to add a schema to the RDD you created in the previous code block. <br> \n",
    "Use first the StructField method, following these steps:<br>\n",
    "1- Define your schema columns as a string<br>\n",
    "2- Build the schema object using StructField<br>\n",
    "3- Apply the schema object to the RDD<br>\n",
    "\n",
    "Note: The cell below is missing some code and will not run properly until you add in some missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# The schema is encoded in a string. Complete the string below\n",
    "schemaString = \"\"\n",
    "\n",
    "# MissingType() should be either StringType() or IntegerType(). Please replace as required.\n",
    "fields = [StructField(field_name, MissingType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaExample = sqlContext.createDataFrame(use_your_rdd_name_here, schema)\n",
    "\n",
    "# Register the DataFrame as a table. Add table name below as parameter to registerTempTable.\n",
    "schemaExample.registerTempTable(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, write some SQL statements to verify that you successfully added a schema to your RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run some SQL statements on your newly created DataFrame and display the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "### Reading from an external data source\n",
    "If you have time, this is a good example to show you how to read from other datasources.  <br><br>\n",
    "In a different browser tab, create a dashDB service, add credentials and come back to this notebook. <br>Each dashDB instance in bluemix is created with a \"GOSALES\" set of tables which we can reuse for the purpose of this example. (You can create your own table if you wish...)<br><br>Replace the Xs in the cell below with proper credentials and verify access to dashDB tables.<br><br>\n",
    "You can read from any database that you can connect to through jdbc.  Here is the [documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salesDF = sqlContext.read.format('jdbc').\\\n",
    "          options(url='jdbc:db2://dashdb-entry-yp-xxxxx.services.dal.bluemix.net:50000/BLUDB:user=xxxxx;password=xxxxxx;',\\\n",
    "                  dbtable='GOSALES.BRANCH').load()\n",
    "salesDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}